<div align="center">
    # AI Security Labs Handbook ğŸ›¡ï¸

A comprehensive hands-on laboratory series demonstrating production-grade AI security patterns across different AI architectures: simple LLMs, RAG systems, and agentic AI.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.11-blue.svg)
![Labs](https://img.shields.io/badge/labs-3-green.svg)
</div>

## ğŸ¯ Overview

This handbook provides **practical, runnable examples** of AI security implementations that work across different deployment models - whether you're using local open-source models (with Ollama), cloud APIs (OpenAI, Anthropic), or proprietary in-house models.

### ğŸ”‘ Core Concept: Provider-Agnostic Security
**Defense-in-Depth Architecture**: Multiple independent security layers that work regardless of your LLM provider.

<table>
<tr>
<td colspan="2" align="center"><strong>ğŸ¯ Your Application Layer</strong></td>
</tr>
<tr>
<td colspan="2" align="center">â¬‡ï¸</td>
</tr>
<tr>
<td colspan="2" align="center" bgcolor="#fff4e6"><strong>ğŸ›¡ï¸ Security Gateway (Provider-Agnostic)</strong></td>
</tr>
<tr>
<td width="50%" valign="top">
<strong>ğŸ“¥ Pre-Processing</strong>
<ul>
<li>âœ“ PII Detection & Masking</li>
<li>âœ“ Prompt Injection Detection</li>
<li>âœ“ Policy Enforcement (OPA)</li>
<li>âœ“ Input Validation</li>
</ul>
</td>
<td width="50%" valign="top">
<strong>ğŸ“¤ Post-Processing</strong>
<ul>
<li>âœ“ Output Sanitization</li>
<li>âœ“ Response DLP</li>
<li>âœ“ Provenance Tracking</li>
</ul>
</td>
</tr>
<tr>
<td colspan="2" align="center">â¬‡ï¸</td>
</tr>
<tr>
<td colspan="2" align="center" bgcolor="#e8f5e9">
<strong>ğŸ”Œ LLM Provider (Interchangeable)</strong><br/>
Ollama (Local) â€¢ OpenAI/Anthropic (Cloud) â€¢ Azure OpenAI (Enterprise) â€¢ Custom In-House
</td>
</tr>
</table>

**The key insight**: Security processors operate on requests/responses as data structures - they don't care where the LLM lives or how it's called. Only the thin provider layer changes.

## ğŸ§ª Laboratory Series

### Lab 01: PII-Safe Summarizer âœ… Complete
**Focus**: Simple LLM + Security Fundamentals

A document summarization service with comprehensive security controls.

**Security Features**:
- Data Loss Prevention (PII masking)
- Prompt injection detection
- Role-based and Attribute-based access control (RBAC/ABAC)
- Performance monitoring

**[ğŸ‘‰ Go to Lab 01](labs/01-pii-safe-summarizer/README.md)**

**Learn**: Foundation security patterns that apply to all AI systems

---

### Lab 02: Secure RAG Copilot ğŸš§ Coming Soon
**Focus**: Retrieval-Augmented Generation + Context Security
**Learn**: RAG-specific threats and how to mitigate them

---

### Lab 03: Observability and Governance in Agentic AI ğŸš§ Coming Soon
**Focus**: Agentic AI + Observability, Governance and Context
**Learn**: Controlling autonomous AI systems safely without slowing adoption

---
See individual lab READMEs for specific setup instructions.#

## ğŸ“ What You'll Learn
**Security Fundamentals (Lab01)**

* Defense-in-depth architecture: Multiple independent security layers
* Input validation & sanitization, output filtering: Handling sensitive vs non-sensitive data
* Policy-based access control: Declarative access control with OPA/Rego
* Audit trails & observability: Performance monitoring and audit trails

## ğŸ“š Further Reading
* OWASP Top 10 for LLMs
* NIST AI Risk Management Framework
* Anthropic's Claude Safety Best Practices

## ğŸ™ Acknowledgments
Built to demonstrate enterprise-grade AI security patterns for educational purposes.
