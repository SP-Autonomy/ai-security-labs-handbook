# Lab 01: PII-Safe Summarizer ğŸ”

<div align="center">

**Foundation Security Patterns for LLM Applications**

[![Lab Status](https://img.shields.io/badge/status-complete-success.svg)](.)
[![Difficulty](https://img.shields.io/badge/difficulty-beginner-green.svg)](.)
[![Time](https://img.shields.io/badge/time-2--3%20hours-blue.svg)](.)

[ğŸ”’ Security Layers](#security-layers) â€¢ [ğŸš€ Setup](#-setup) â€¢ [ğŸ§ª Test Scenarios](#-test-scenarios) â€¢ [ğŸ“Š Performance](#-performance-analysis) â€¢ [âš–ï¸ Cost-Benefit Analysis](#cost-benefit-analysis)

</div>

---

## ğŸ¯ Overview

A document summarization service demonstrating **foundational AI security patterns** that apply to all LLM applications. This lab teaches you to build security layers that work regardless of your LLM provider.

### What This Lab Covers

<table>
<tr>
<td width="50%">

**ğŸ”’ Security Features:**
- âœ… Data Loss Prevention (DLP)
- âœ… Prompt Injection Detection
- âœ… Role & Attribute-Based Access Control (RBAC/ABAC)
- âœ… Performance Monitoring
- âœ… Audit Trail & Provenance

</td>
<td width="50%">

**ğŸ’¼ Real-World Use Cases:**
- ğŸ¢ Financial services (protect customer PII)
- ğŸ¥ Healthcare (HIPAA compliance)
- ğŸ›ï¸ Government (classified data handling)
- ğŸ’¼ Enterprise SaaS (multi-tenant security)

</td>
</tr>
</table>

### Learning Objectives

By completing this lab, you will:

1. **Understand** defense-in-depth architecture for AI applications
2. **Implement** PII detection and masking at scale
3. **Deploy** policy-based access control with OPA
4. **Monitor** security and performance in production
5. **Apply** these patterns to any LLM provider

---

## ğŸ¯ Threat Landscape & Security Posture

### MITRE ATLAS Threat Model

This lab addresses threats from the [MITRE ATLASâ„¢](https://atlas.mitre.org/) (Adversarial Threat Landscape for Artificial-Intelligence Systems) framework - the industry standard for AI/ML security threat modeling.

---

### ğŸ”´ Risk Heat Map

Risk assessment based on **Likelihood Ã— Impact** without security controls:

<table>
<thead>
<tr>
<th width="25%">Threat Category</th>
<th width="15%">Likelihood</th>
<th width="15%">Impact</th>
<th width="15%">Risk Level</th>
<th width="30%">Business Consequence</th>
</tr>
</thead>
<tbody>
<tr style="background-color: #ffebee;">
<td><strong>Prompt Injection</strong></td>
<td>ğŸ”´ High</td>
<td>ğŸ”´ Critical</td>
<td>ğŸ”´ <strong>CRITICAL</strong></td>
<td>Model manipulation, unauthorized actions, policy bypass</td>
</tr>
<tr style="background-color: #ffebee;">
<td><strong>PII Exfiltration</strong></td>
<td>ğŸ”´ High</td>
<td>ğŸ”´ Critical</td>
<td>ğŸ”´ <strong>CRITICAL</strong></td>
<td>Regulatory fines (GDPR/CCPA), reputational damage, customer trust loss</td>
</tr>
<tr style="background-color: #fff3e0;">
<td><strong>Unauthorized Access</strong></td>
<td>ğŸŸ¡ Medium</td>
<td>ğŸ”´ High</td>
<td>ğŸŸ  <strong>HIGH</strong></td>
<td>Data breach, compliance violations, insider threats</td>
</tr>
<tr style="background-color: #fff3e0;">
<td><strong>Model Abuse</strong></td>
<td>ğŸŸ¡ Medium</td>
<td>ğŸŸ¡ Medium</td>
<td>ğŸŸ¡ <strong>MEDIUM</strong></td>
<td>Resource exhaustion, cost overruns, service degradation</td>
</tr>
<tr style="background-color: #e8f5e9;">
<td><strong>Training Data Poisoning</strong></td>
<td>ğŸŸ¢ Low</td>
<td>ğŸŸ¡ Medium</td>
<td>ğŸŸ¢ <strong>LOW</strong></td>
<td>Not applicable - using pre-trained models only</td>
</tr>
</tbody>
</table>

> **Risk Calculation**: Critical (High Likelihood Ã— Critical Impact) | High (Medium Ã— High) | Medium (Medium Ã— Medium) | Low (Low Ã— Medium)

---

### ğŸ›¡ï¸ MITRE ATLAS Technique Coverage

Mapping of security controls to MITRE ATLAS adversarial tactics and techniques:

<table>
<thead>
<tr>
<th width="20%">ATLAS Tactic</th>
<th width="25%">Technique ID</th>
<th width="25%">Security Control</th>
<th width="15%">Coverage</th>
<th width="15%">Residual Risk</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="3"><strong>ML Attack Staging</strong><br/><small>AML.TA0000</small></td>
<td><strong>Prompt Injection</strong><br/>AML.T0051.000</td>
<td>Injection Guard (Pattern Detection)</td>
<td>ğŸŸ¢ 85%</td>
<td>ğŸŸ¡ Medium</td>
</tr>
<tr>
<td><strong>LLM Jailbreak</strong><br/>AML.T0054.000</td>
<td>Injection Guard + Policy Gate</td>
<td>ğŸŸ¡ 70%</td>
<td>ğŸŸ¡ Medium</td>
</tr>
<tr>
<td><strong>Adversarial Data</strong><br/>AML.T0043.002</td>
<td>DLP Pre-Processing</td>
<td>ğŸŸ¢ 90%</td>
<td>ğŸŸ¢ Low</td>
</tr>
<tr>
<td rowspan="2"><strong>ML Model Access</strong><br/><small>AML.TA0001</small></td>
<td><strong>API Access</strong><br/>AML.T0040.000</td>
<td>Policy Gate (OPA ABAC)</td>
<td>ğŸŸ¢ 95%</td>
<td>ğŸŸ¢ Low</td>
</tr>
<tr>
<td><strong>Unauthorized Use</strong><br/>AML.T0040.001</td>
<td>Policy Gate + Authentication</td>
<td>ğŸŸ¢ 90%</td>
<td>ğŸŸ¢ Low</td>
</tr>
<tr>
<td rowspan="2"><strong>Exfiltration</strong><br/><small>AML.TA0010</small></td>
<td><strong>Inference API Exfiltration</strong><br/>AML.T0024.000</td>
<td>DLP Pre + DLP Post</td>
<td>ğŸŸ¢ 95%</td>
<td>ğŸŸ¢ Low</td>
</tr>
<tr>
<td><strong>Model Inversion</strong><br/>AML.T0024.001</td>
<td>Rate Limiting (Future)</td>
<td>ğŸ”´ 0%</td>
<td>ğŸ”´ High</td>
</tr>
<tr>
<td><strong>Impact</strong><br/><small>AML.TA0040</small></td>
<td><strong>Erode ML Model Integrity</strong><br/>AML.T0048.000</td>
<td>Provenance Tracking + Audit Logs</td>
<td>ğŸŸ¢ 80%</td>
<td>ğŸŸ¢ Low</td>
</tr>
</tbody>
</table>

**Coverage Ratings:**
- ğŸŸ¢ **>80%**: Strong protection with minimal residual risk
- ğŸŸ¡ **60-80%**: Moderate protection, additional controls recommended
- ğŸ”´ **<60%**: Significant gap, prioritize enhancement

---

### ğŸ“š References

- **MITRE ATLASâ„¢**: [atlas.mitre.org](https://atlas.mitre.org/)
- **OWASP Top 10 for LLMs**: [owasp.org/www-project-top-10-for-large-language-model-applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- **NIST AI Risk Management Framework**: [nist.gov/itl/ai-risk-management-framework](https://www.nist.gov/itl/ai-risk-management-framework)
- **ENISA Threat Landscape for AI**: [enisa.europa.eu](https://www.enisa.europa.eu/)


> **ğŸ’¡ Professional Note**: This threat model follows industry best practices from organizations deploying production LLM systems, including OpenAI, Anthropic, Google DeepMind, and Fortune 500 enterprises. The layered defense approach aligns with **Zero Trust Architecture** principles and **NIST Cybersecurity Framework** guidelines.

---

### ğŸ“ Security Assumptions & Threat Model Scope

**In Scope:**
- âœ… Application-layer attacks (prompt injection, data exfiltration)
- âœ… Unauthorized access attempts (authentication/authorization bypass)
- âœ… PII disclosure (intentional or accidental)
- âœ… Policy violations (insider threats with valid credentials)

**Out of Scope:**
- âŒ Infrastructure attacks (DDoS, network intrusion) - handled by platform layer
- âŒ Model training attacks (data poisoning) - using pre-trained models
- âŒ Physical security - assumed secure deployment environment
- âŒ Supply chain attacks - dependency management separate concern

**Assumptions:**
1. **Trusted Platform**: Underlying infrastructure (OS, network, hardware) is secure
2. **Model Integrity**: Pre-trained LLM models are from trusted sources
3. **Secure Channel**: HTTPS/TLS for API communication (not implemented in lab)
4. **Authentication**: User authentication handled upstream (simulated in lab)

---

### ğŸ“ˆ Compliance & Regulatory Alignment

Security controls map to regulatory requirements:

| Regulation | Requirement | Control Implementation | Status |
|-----------|-------------|----------------------|--------|
| **GDPR Art. 32** | Data protection by design | DLP Pre + Post, Policy Gate | âœ… Compliant |
| **CCPA Â§ 1798.150** | Reasonable security measures | Defense-in-depth, encryption ready | âœ… Compliant |
| **HIPAA Â§ 164.312** | Access controls & audit logs | ABAC policies, provenance tracking | âœ… Compliant |
| **SOC 2 Type II** | Security monitoring & logging | Comprehensive audit trails | âœ… Compliant |
| **ISO 27001** | Information security management | Risk-based controls, documentation | âœ… Compliant |
| **NIST AI RMF** | AI risk management | ATLAS threat modeling, layered defenses | âœ… Compliant |

## ğŸ—ï¸ Architecture

### Security Processing Chain
```mermaid
graph LR
    A[User Request] --> B[DLP Pre]
    B --> C[Injection Guard]
    C --> D[Policy Gate]
    D --> E{Allowed?}
    E -->|Yes| F[LLM Call]
    E -->|No| G[Block & Log]
    F --> H[DLP Post]
    H --> I[Add Provenance]
    I --> J[Response]
    
    style A fill:#2d2d2d,stroke:#666,stroke-width:2px,color:#fff
    style B fill:#918f03,stroke:#666,stroke-width:2px,color:#000
    style C fill:#918f03,stroke:#666,stroke-width:2px,color:#000
    style D fill:#918f03,stroke:#666,stroke-width:2px,color:#000
    style E fill:#1a1a1a,stroke:#666,stroke-width:2px,color:#fff
    style F fill:#2d2d2d,stroke:#666,stroke-width:2px,color:#fff
    style G fill:#8b0000,stroke:#666,stroke-width:2px,color:#fff
    style H fill:#918f03,stroke:#666,stroke-width:2px,color:#000
    style I fill:#2d2d2d,stroke:#666,stroke-width:2px,color:#fff
    style J fill:#2d2d2d,stroke:#666,stroke-width:2px,color:#fff
```
### Security Layers

| Layer | Purpose | Latency | Blocks On |
|-------|---------|---------|-----------|
| **1. DLP Pre** | Mask PII in input | <1ms | - |
| **2. Injection Guard** | Detect prompt attacks | <1ms | Suspicious patterns |
| **3. Policy Gate** | Enforce ABAC rules | ~13ms | Role + sensitivity mismatch |
| **4. LLM Call** | Generate response | ~5-20s | - |
| **5. DLP Post** | Mask PII in output | <1ms | - |
| **6. Provenance** | Add audit metadata | <1ms | - |

---

## ğŸš€ Setup

### Prerequisites

- âœ… Python 3.11+
- âœ… [Ollama](https://ollama.com/download) installed (or OpenAI API key)
- âœ… [OPA](https://www.openpolicyagent.org/docs/latest/#running-opa) installed

### Quick Start

**Step 1: Install Dependencies**
```bash
# From repository root
cd ai-security-labs-handbook
source .venv/bin/activate  # If not already activated
pip install -r requirements.txt
```
**Step 2: Configure Environment Variables**
```bash
nano .env
MODEL_PROVIDER=ollama
GEN_MODEL=llama3.2:1b
OLLAMA_HOST=http://localhost:11434
OPA_URL=http://localhost:8181/v1/data/ai/policy/allow
```
**Step 3: Pull Model (Ollama users)**
```bash
ollama pull llama3.2:1b
```
**Step 4: Start Services**
```bash
ollama serve
make run-opa
make run-api
```

## ğŸ§ª Test Scenarios
### Running Tests
**Run all tests:**
```bash
make test-all
```
**Run individual tests:**
```bash
make test-malicious-contractor           # Test 1
make test-benign-employee                # Test 2
make test-sensitive-employee-denied      # Test 3
make test-sensitive-employee-approved    # Test 4
```

**Test Matrix**
<table>
<thead>
<tr>
<th width="25%">Scenario</th>
<th width="15%">Role</th>
<th width="15%">Clearance</th>
<th width="15%">Has PII</th>
<th width="15%">Has Injection</th>
<th width="15%">Expected</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Test 1</strong><br/>Malicious + Contractor</td>
<td>contractor</td>
<td>-</td>
<td>âœ… Yes</td>
<td>âœ… Yes</td>
<td>âŒ BLOCKED<br/><code>prompt_injection_suspected</code></td>
</tr>
<tr>
<td><strong>Test 2</strong><br/>Benign + Employee</td>
<td>employee</td>
<td>-</td>
<td>âŒ No</td>
<td>âŒ No</td>
<td>âœ… SUCCESS</td>
</tr>
<tr>
<td><strong>Test 3</strong><br/>Sensitive Information + Regular Employee</td>
<td>employee</td>
<td>none</td>
<td>âœ… Yes</td>
<td>âŒ No</td>
<td>âŒ BLOCKED<br/><code>policy_denied</code></td>
</tr>
<tr>
<td><strong>Test 4</strong><br/>Sensitive Information + Approved Employee</td>
<td>employee</td>
<td>pii_approved</td>
<td>âœ… Yes</td>
<td>âŒ No</td>
<td>âœ… SUCCESS with PII masked</td>
</tr>
</tbody>
</table>

## ğŸ¯ Expected Results
<details>
<summary><b>âŒ Test 1: BLOCKED - Malicious Contractor</b></summary>
  <img width="1029" height="620" alt="Malicious Contractor Prompt" src="https://github.com/user-attachments/assets/5f6bb24d-76cc-4382-88bb-d431c1701db7" />
</details>
<details><summary><b>âœ… Test 2: SUCCESS - Benign Employee</b></summary>
  <img width="1360" height="1707" alt="Benign Employee Prompt" src="https://github.com/user-attachments/assets/858f597e-1abb-4a82-95ae-2fdc7e9a61c5" />
</details>
<details><summary><b>âŒ Test 3: BLOCKED - Sensitive Information + Regular Employee</b></summary>
  <img width="1119" height="752" alt="Regular Employee Sensitive Content Prompt" src="https://github.com/user-attachments/assets/2b30503d-6048-41ec-aa0e-181ac8fd9fa9" />
</details>
<details><summary><b>âœ… Test 4: SUCCESS - Sensitive Information + Approved Employee</b></summary>
  <img width="1360" height="1771" alt="Approved Employee Sensitive Content Prompt" src="https://github.com/user-attachments/assets/5a5ea220-6b08-4c13-a74d-9edc0509fdb8" />
</details>

## ğŸ“Š Performance Analysis

### Actual Test Results

Based on real measurements from the running system:

#### Test 2: Complete Request Flow (Success)

| Stage | Latency (ms) | % of Total | Cumulative |
|-------|--------------|------------|------------|
| **dlp_pre** | 0.0 | <0.01% | 0.0ms |
| **injection_guard** | 0.2 | <0.01% | 0.2ms |
| **policy_gate** | 12.4 | 0.08% | 12.6ms |
| **llm_call** | 16,253.8 | 99.91% | 16,266.4ms |
| **dlp_post** | 0.6 | <0.01% | 16,267.0ms |
| **add_provenance** | 0.0 | <0.01% | 16,267.0ms |
| **TOTAL** | **16,267ms** | **100%** | - |

> **ğŸ’¡ Key Finding:** Security overhead is only **13.2ms (0.08%)** of total request time.

### Performance Insights

**Blocked requests never reach the LLM**, saving:
- âš¡ **16+ seconds** of processing time
- ğŸ’° **API costs** (if using cloud LLMs)
- ğŸ”’ **Potential security breaches**

---

## âš–ï¸ Cost-Benefit Analysis

### Security Benefits

- âœ… **Prevents data leaks** - PII never reaches LLM
- âœ… **Blocks attacks** - Injection attempts stopped in <1ms
- âœ… **Enforces policies** - Authorization in ~13ms
- âœ… **Audit compliance** - Every request tracked

### Performance Cost

| Without Security | With Security | Overhead |
|-----------------|---------------|----------|
| 16,254ms | 16,267ms | **+13ms (0.08%)** |

**ROI:** Comprehensive security for **0.08% performance cost** is exceptional.

---

## ğŸ“ What You'll Learn

### Technical Skills

- âœ… **FastAPI Development** - Async API design, file uploads, form data
- âœ… **Security Engineering** - DLP, injection detection, access control
- âœ… **Policy as Code** - Writing and testing OPA/Rego policies
- âœ… **LLM Integration** - Provider abstraction, error handling
- âœ… **Observability** - Performance monitoring, structured logging

### Security Concepts

- ğŸ›¡ï¸ **Defense-in-Depth** - Multiple independent security layers
- ğŸ”’ **Zero Trust** - Explicit authorization for every request
- ğŸ“Š **Data Classification** - Sensitive vs non-sensitive handling
- ğŸ¯ **Principle of Least Privilege** - Minimal access by default
- ğŸ“ **Audit Trails** - Compliance and incident response

### Architecture Patterns

- ğŸŒ **Gateway Pattern** - Centralized security enforcement
- ğŸ”— **Chain of Responsibility** - Composable processors
- ğŸ”Œ **Provider Abstraction** - Model-agnostic implementations
- ğŸ“œ **Policy Abstraction** - Separating policy from code

---

## ğŸš€ Next Steps

### Completed Lab 01? ğŸ‰

**You've mastered:**
- âœ… Core security patterns for LLM applications
- âœ… Provider-agnostic architecture
- âœ… Policy-based access control
- âœ… Observability and audit trails

### Continue Learning:

1. **[ğŸ“– Lab 02: Secure RAG Copilot](../02-secure-rag-copilot/)** - Learn RAG-specific security
2. **[ğŸ“– Lab 03: Agentic AI Governance and Observability](../03-governed-ai-agent/)** - Master agentic AI security